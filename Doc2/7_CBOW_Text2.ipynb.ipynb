{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f089b4-0a8b-4cfc-ba60-f2c5332a1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 7: Continuous Bag of Words (CBOW) Model – Document 2\n",
    "\n",
    "# Step 1: Import Libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Lambda, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Prepare Sample Text Data\n",
    "text = \"artificial intelligence is transforming industries and creating opportunities across the world\"\n",
    "text = text.lower().split()\n",
    "\n",
    "# Step 3: Tokenize Words and Create Vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "vocab_size = len(word2id) + 1\n",
    "\n",
    "# Step 4: Generate Training Data for CBOW\n",
    "window_size = 2\n",
    "data = []\n",
    "\n",
    "for i in range(window_size, len(text) - window_size):\n",
    "    context = []\n",
    "    for j in range(-window_size, window_size + 1):\n",
    "        if j != 0:\n",
    "            context.append(word2id[text[i + j]])\n",
    "    target = word2id[text[i]]\n",
    "    data.append((context, target))\n",
    "\n",
    "X = np.array([x for x, _ in data])\n",
    "y = np.array([y for _, y in data])\n",
    "\n",
    "# Step 5: Define CBOW Model Architecture\n",
    "embedding_dim = 10\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=2 * window_size),\n",
    "    Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,)),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Step 6: Compile the Model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Step 7: Train the Model\n",
    "history = model.fit(X, y, epochs=100, verbose=0)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Step 8: Plot Training Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Display Word Embeddings\n",
    "weights = model.get_weights()[0]\n",
    "for word, idx in word2id.items():\n",
    "    print(f\"{word}: {weights[idx]}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# EXPLANATION OF EACH STEP\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Import Libraries\n",
    "# - TensorFlow/Keras for the neural network.\n",
    "# - Tokenizer for word-to-index conversion.\n",
    "# - Matplotlib for loss visualization.\n",
    "\n",
    "# Step 2: Prepare Sample Text Data\n",
    "# - A short custom sentence (document 2) is tokenized into words.\n",
    "# - Used as the training data for CBOW.\n",
    "\n",
    "# Step 3: Tokenize Words and Create Vocabulary\n",
    "# - Each unique word is given an integer ID.\n",
    "# - Vocabulary size = total unique words + 1.\n",
    "\n",
    "# Step 4: Generate Training Data\n",
    "# - CBOW predicts the target word from the surrounding context.\n",
    "# - Window size = 2 → 2 words before and 2 words after the target.\n",
    "\n",
    "# Step 5: Define CBOW Model Architecture\n",
    "# - Embedding layer: converts words into dense numeric vectors.\n",
    "# - Lambda layer: computes average of context embeddings.\n",
    "# - Dense layer: outputs probability distribution for predicting target word.\n",
    "\n",
    "# Step 6: Compile the Model\n",
    "# - Loss: sparse categorical crossentropy.\n",
    "# - Optimizer: Adam for adaptive learning.\n",
    "\n",
    "# Step 7: Train the Model\n",
    "# - Model learns context-word relationships for 100 epochs.\n",
    "\n",
    "# Step 8: Plot Training Loss\n",
    "# - Displays the loss reduction over epochs → ensures learning stability.\n",
    "\n",
    "# Step 9: Display Word Embeddings\n",
    "# - Prints learned word vector representations.\n",
    "# - Words with similar context appear closer in vector space.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# VIVA QUESTIONS\n",
    "# ------------------------------------------------------------\n",
    "# Q1. What is the purpose of the CBOW model?\n",
    "#     -> To predict a target word given its surrounding context words.\n",
    "# Q2. What is the input and output of CBOW?\n",
    "#     -> Input: context words; Output: target word.\n",
    "# Q3. What is a window size in CBOW?\n",
    "#     -> Number of words around the target word considered as context.\n",
    "# Q4. What is the role of the Embedding layer?\n",
    "#     -> Converts integer word indices into dense vector representations.\n",
    "# Q5. Why use a mean (Lambda layer) in CBOW?\n",
    "#     -> Combines multiple context embeddings into a single averaged vector.\n",
    "# Q6. What loss function is used?\n",
    "#     -> Sparse categorical crossentropy.\n",
    "# Q7. What optimizer is used?\n",
    "#     -> Adam optimizer.\n",
    "# Q8. What is the advantage of CBOW over Skip-Gram?\n",
    "#     -> CBOW is faster for large datasets, predicting one center word per context.\n",
    "# Q9. What does the output layer (Softmax) do?\n",
    "#     -> Produces probability distribution for all possible target words.\n",
    "# Q10. Applications of CBOW?\n",
    "#     -> Word embeddings, NLP tasks like text classification, and semantic analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
