{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e710ead-98aa-45a9-b20a-c7a49a457051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 8: Continuous Bag of Words (CBOW) Model – Document 3\n",
    "\n",
    "# Step 1: Import Libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Lambda, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Prepare Sample Text Data\n",
    "text = \"deep learning is a subset of machine learning that uses neural networks to model complex patterns\"\n",
    "text = text.lower().split()\n",
    "\n",
    "# Step 3: Tokenize Words and Create Vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "vocab_size = len(word2id) + 1\n",
    "\n",
    "# Step 4: Generate Training Data for CBOW\n",
    "window_size = 2\n",
    "data = []\n",
    "\n",
    "for i in range(window_size, len(text) - window_size):\n",
    "    context = []\n",
    "    for j in range(-window_size, window_size + 1):\n",
    "        if j != 0:\n",
    "            context.append(word2id[text[i + j]])\n",
    "    target = word2id[text[i]]\n",
    "    data.append((context, target))\n",
    "\n",
    "X = np.array([x for x, _ in data])\n",
    "y = np.array([y for _, y in data])\n",
    "\n",
    "# Step 5: Define CBOW Model Architecture\n",
    "embedding_dim = 10\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=2 * window_size),\n",
    "    Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,)),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Step 6: Compile the Model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Step 7: Train the Model\n",
    "history = model.fit(X, y, epochs=100, verbose=0)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Step 8: Plot Training Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Display Word Embeddings\n",
    "weights = model.get_weights()[0]\n",
    "for word, idx in word2id.items():\n",
    "    print(f\"{word}: {weights[idx]}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# EXPLANATION OF EACH STEP\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Import Libraries\n",
    "# - TensorFlow/Keras for CBOW implementation.\n",
    "# - Tokenizer for converting words to numeric IDs.\n",
    "# - Matplotlib for plotting training loss.\n",
    "\n",
    "# Step 2: Prepare Sample Text Data\n",
    "# - Custom text (document 3) about deep learning is split into words for training.\n",
    "\n",
    "# Step 3: Tokenize Words and Create Vocabulary\n",
    "# - Tokenizer assigns a unique integer to every unique word.\n",
    "# - Vocabulary size = total number of unique words + 1.\n",
    "\n",
    "# Step 4: Generate Training Data\n",
    "# - CBOW predicts a target word using words around it (context).\n",
    "# - Window size = 2 → two words before and two after.\n",
    "# - Example: for “subset”, context = [\"learning\", \"is\", \"of\", \"machine\"].\n",
    "\n",
    "# Step 5: Define CBOW Model Architecture\n",
    "# - Embedding layer: converts word IDs to dense vectors.\n",
    "# - Lambda layer: averages embeddings of context words.\n",
    "# - Dense layer: uses Softmax to predict the most probable target word.\n",
    "\n",
    "# Step 6: Compile the Model\n",
    "# - Loss: sparse categorical crossentropy for predicting a single class.\n",
    "# - Optimizer: Adam (efficient gradient-based optimization).\n",
    "\n",
    "# Step 7: Train the Model\n",
    "# - Model learns to predict target words from their context for 100 epochs.\n",
    "\n",
    "# Step 8: Plot Training Loss\n",
    "# - Displays how the model’s loss decreases across epochs, confirming learning progress.\n",
    "\n",
    "# Step 9: Display Word Embeddings\n",
    "# - Displays 10-dimensional embeddings learned for each word.\n",
    "# - Similar words have similar vector values.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# VIVA QUESTIONS\n",
    "# ------------------------------------------------------------\n",
    "# Q1. What is the CBOW model?\n",
    "#     -> A neural network that predicts a target word given its surrounding context words.\n",
    "# Q2. What dataset/text is used here?\n",
    "#     -> A custom text about deep learning and neural networks.\n",
    "# Q3. What is the difference between CBOW and Skip-Gram?\n",
    "#     -> CBOW predicts target word from context; Skip-Gram predicts context words from target.\n",
    "# Q4. What does the Embedding layer do?\n",
    "#     -> Converts integer tokens into continuous vector representations.\n",
    "# Q5. Why use the mean (Lambda layer)?\n",
    "#     -> Averages multiple context word embeddings into one feature vector.\n",
    "# Q6. What loss and optimizer are used?\n",
    "#     -> Loss = sparse categorical crossentropy; Optimizer = Adam.\n",
    "# Q7. How many epochs are used for training?\n",
    "#     -> 100 epochs for stable learning of relationships.\n",
    "# Q8. What is the purpose of Softmax layer?\n",
    "#     -> Outputs probability for each word being the target.\n",
    "# Q9. What are word embeddings?\n",
    "#     -> Dense vector representations capturing meaning and relationships of words.\n",
    "# Q10. Applications of CBOW embeddings?\n",
    "#     -> Used in NLP tasks like text classification, chatbots, translation, and search engines.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
